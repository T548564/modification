% !TeX root = ../main.tex
% -*- coding: utf-8 -*-


\chapter{相关技术} \label{2}

本章主要介绍深度神经网络模型知识产权领域的相关技术。首先，介绍了深度神经网络的基本结构和原理以及本文涉及到的相关术语，便于理解本文提出的方法。然后，着重介绍了对抗性攻击和生成对抗网络的原理，为近边界数据的生成提供技术基础。最后，介绍模型水印和指纹两种主流知识产权保护方法的原理和实现方式。

\section{深度神经网络}

人工神经网络是一种模拟生物神经系统的信息处理模型，旨在通过一系列相互连接的神经元（网络中的节点）来处理复杂的数据。它可以被看作是由许多基本单元组成的网络，每个基本单元都可以接收来自其他基本单元的信号并输出一个新的信号。通常，神经网络由输入层，隐藏层和输出层组成。如图\ref{深度神经网络结构图}所示，如果神经网络有多个隐藏层，则被称为深度神经网络。深度神经网络的隐藏层一般由多个卷积层，池化层，全连接层，Dropout层和Softmax层构成，这些层能够提取数据的高级特征，并对复杂的非线性数据进行有效建模。在神经网络的训练过程中，数据输入到输入层，通过每一层后，每层会提取出一些抽象特征作为下一层的输入，最终由输出层输出最终结果。
	
\begin{figure}[htbp]%%图,[htbp]是浮动格式
	\centering
	\setlength{\abovecaptionskip}{3mm} %图片标题与图片距离
%	\vspace{-2mm}
	\setlength{\belowcaptionskip}{-3mm} %调整图片标题与下文距离
	\includegraphics[width=1\linewidth]{深度神经网络结构.pdf}
	\caption{深度神经网络结构图}
	\label{深度神经网络结构图}
	%	\vspace{-3mm}  %调整图片标题与下文距离，与\setlength{\belowcaptionskip}{-3mm}等效。
	\end {figure}

深度神经网络一种非常强大的非线性数学函数，能够将一组输入变量转化为一组输出变量。每个神经元都有对应的权重和偏置参数，这些参数控制着输入的精确转化，从而实现了高度准确的数据建模和预测。在DNN模型的训练过程中，需要确定神经网络中每个神经元的权重和偏置参数。这个过程可以通过梯度下降算法和反向传播来实现。梯度下降算法通过最小化损失函数来更新参数，反向传播则是用来计算参数的梯度，从而实现参数的更新。神经网络模型的学习和训练是一个需要大量计算资源的过程，然而一旦权重和偏置参数确定，DNN模型就可以快速地处理相似类型的新数据，识别并提取海量数据中的复杂特征。

以下是本文中涉及到的\textbf{相关术语}：

1）源模型。源模型也称作目标模型、原始模型，是指模型所有者在私有或公开数据集上，消耗大量计算资源和人力资源训练出的高性能DNN模型，可能因学术研究放置在开源社区，或者作为商用给用户提供远程API。

2）可疑模型。可疑模型也称作盗窃模型、替代模型、派生模型，是指该模型可能是通过模型窃取攻击方法从源模型派生的模型，判断一个可疑模型是否是从源模型派生是模型知识产权保护领域的主要目标。

3）白盒环境。白盒环境是指能够获得DNN模型的所有知识，包括训练集，训练方式，模型参数，模型结构等。

4）黑盒环境。黑盒环境指不清楚模型内部参数和结构等，但可以通过模型提供的API获得指定输入的输出。

5）\textbf{验证模型所有权}。验证模型所有权指通过检测有无特定的水印或者指纹是否匹配的方式解决模型的所有权问题，检测到特定的水印或者指纹匹配说明验证所有权成功。

6）\textbf{推断模型所有权}。推断模型所有权是本文提出的新概念，指通过数据在模型上的最优性解决模型的所有权问题，最优数据提供者推断获得所有权。




\section{对抗性攻击}

\subsection{对抗性样本}
 
对抗性样本的概念是Szegedy等人\cite{szegedy2013intriguing}提出的。这篇文章中指出，通常情况下，一个性能良好的神经网路模型具备优异的泛化能力，即对输入的微小随机扰动具较强有鲁棒性，从而保证了模型在处理图像时的类别预测准确性。然而，如果对图像添加特定的非随机扰动，使得损失函数的值上升，那么DNN模型的预测结果就可以随意改变。这些难以被人眼察觉但足以使得模型输出错误类别的图像样本即为对抗性样本。

\begin{figure}[htbp]%%图,[htbp]是浮动格式
	\centering
	\setlength{\abovecaptionskip}{3mm} %图片标题与图片距离
	%	\vspace{-2mm}
	\setlength{\belowcaptionskip}{-3mm} %调整图片标题与下文距离
	\includegraphics[width=0.9\linewidth]{对抗攻击示意图.pdf}
	\caption{对抗性攻击示意图}
	\label{对抗性攻击示意图}
	%	\vspace{-3mm}  %调整图片标题与下文距离，与\setlength{\belowcaptionskip}{-3mm}等效。
	\end {figure}

如图\ref{对抗性攻击示意图}所示，狗的图像被添加特定扰动后被分类器识别为猫。添加特定的扰动，甚至可以使模型输出任意类别。

\noindent\textbf{（1） \  黑盒场景   }

用$f:R^m \rightarrow {1,2,...,n}$表示将一张图片映射为$n$个标签的DNN分类器，对一个正常样本$x \in R^m$以及一个错误标签$l$，目标是找到一个最小的扰动$\delta$，使得分类器将样本$x$错误分类为$l$，如式\ref{eq:1}所示：
\begin{equation}
	\label{eq:1}
	\begin{split}
	min&\parallel \delta \parallel_2, \\
	 s.t. \ f(x + \delta) &= l,\ x + \delta \in [0,1]^m
	\end{split}
\end{equation}

其中叠加了扰动的$x+\delta$即为一个对抗性样本。式\ref{eq:1}这种方式通常用于黑盒的场景下，仅根据DNN分类器的输出进行扰动$\delta$的调整。
 
\noindent\textbf{（2）\  白盒场景   }
 
 在白盒场景下，由于知道模型的所有知识，可以根据这些信息来寻找对抗性样本，通常利用DNN分类器的损失函数来寻找对抗性样本。
 
 用$f:R^m \rightarrow {1,2,...,n}$表示将一张图片映射为$n$个标签的DNN分类器，对一个正常样本$x \in R^m$以及它对应的正确标签$y$，目标是找到一个足够小的扰动$\delta:\delta \leq \gamma$，使得加上扰动后的样本输入DNN模型后，损失函数$L$达到最大值，如式\ref{eq:2}所示：
 \begin{equation}
% 	\setlength\abovedisplayshortskip{1mm}
% 	\setlength\belowdisplayshortskip{4mm}
 	\label{eq:2}
 		\delta = arg \mathop{max} \limits_{\delta \leq \gamma} L(f(\theta, x + \delta), y)
% 		\vspace{3mm}
\end{equation}
%\vspace{-16mm}

其中$\theta$是分类器$f$的参数，$x + \delta$是一个扰动后的对抗性样本。
 
 \subsection{对抗性攻击的类别}

对抗性攻击技术是指生成对抗性样本的方法，不同的方法生成对抗性样本的效率，质量也不相同。根据方式的不同，可以分为以下几类：

1）白盒攻击与黑盒攻击。白盒攻击指敌手知道DNN模型的参数和内部结构等信息，利用这些信息发起的攻击。黑盒攻击指敌手仅根据模型的输入输出来发起攻击。

2）有目标攻击和无目标攻击。有目标攻击指对抗性样本的预测类别为敌手指定的类别，例如将一张牛的图片识别为羊，而不能是其他类别，常采取的方式是向各个方向搜索扰动来最大化DNN模型预测特定类上的可能性。无目标攻击指添加扰动来改变原始预测类别，对具体分类类别不做要求。通常来说有两种攻击方式，一种是最小化DNN模型预测正确类的可能性，一种是进行多次不同类别的的有目标攻击，然后在多个对抗性样本中选取扰动最小的。
	
3）单步攻击和迭代攻击。单步攻击指通过一次添加扰动生成对抗性样本，迭代攻击指通过多次迭代添加微小扰动来生成对抗性样本。通常来说迭代攻击的成功率较高，但是相应的算法复杂度更高，效率较低。
	
4）个体攻击和普适性攻击。个体攻击指针对每个样本都需要重新生成扰动，普适性攻击指找到一个通用的扰动，对数据集中的一类数据都叠加该扰动，普适性攻击效率较高，但是寻找通用扰动的难度较大。

\section{生成对抗网络}\label{2.3}

Goodfellow等人\cite{goodfellow2014generative}第一次提出了生成对抗网络(Generative Adversarial Network, GAN)，是一种利用生成模型实现无监督学习的特殊方法。该网络由一个生成器和一个判别器组成，并通过一种相互博弈的方式进行训练。如图\ref{生成对抗网络结构图}所示，首先随机噪声作为生成器的输入，经过生成器转化成和真实图片具有相同维度的图像。使用原始图片和生成图片分别输入到判定器中，训练判定器区分它们的能力。接着，再使用真实图片训练生成器，使之生成的图片尽可能接近真实图片。通过迭代的交替训练，在训练收敛时，最终生成器生成的图片和原始图片在空间分布上基本一致，判定器判定生成图片和原始图片为真的概率均为$1/2$，也就是无法区分生成图片和原始图片。这种相互博弈的过程可视为生成器和判别器之间的对抗，因此称为生成对抗网络。

\begin{figure}[htbp]%%图,[htbp]是浮动格式
	\centering
	\setlength{\abovecaptionskip}{3mm} %图片标题与图片距离
	%	\vspace{-2mm}
	\setlength{\belowcaptionskip}{-3mm} %调整图片标题与下文距离
	\includegraphics[width=0.97\linewidth]{生成对抗网络结构.pdf}
	\caption{生成对抗网络结构图}
	\label{生成对抗网络结构图}
	%	\vspace{-3mm}  %调整图片标题与下文距离，与\setlength{\belowcaptionskip}{-3mm}等效。
	\end {figure}

具体而言，生成器$G$和判别器$D$可视为博弈中的双方，当训练GAN模型时，生成器$G$和判别器$D$通过更新各自的参数使损失达到最小，经过不断迭代优化，最后$G$和$D$达到纳什均衡。GAN的目标函数如式\ref{eq:3}所示，对于原始图片$x$，判别器希望$D(x)$变大，对应于式中的$maxD$，对于生成图片$G(T)$，生成器希望$D(G(T))$变大，即$log(1 - D(G(T))$变小，对应于式中的$minG$，所以GAN的目标函数由两个目标构成。
\begin{equation}
	\label{eq:3}
	\begin{split}
		\mathop{min} \limits_{G} \mathop{max} \limits_{D} V(D, G) &= \mathop{min} \limits_{G} \mathop{max} \limits_{D} E_{x \sim P_{data}(x)}[logD(x)] \\
		&+ E_{T \sim P_{T}(T)}[log(1 - D(G(T)))]
	\end{split}
\end{equation}

其中$x$表示原始图片，$T$表示用于生成样本的随机噪声，GAN对噪声$T$的分布没有特别要求，常用的有高斯分布、均匀分布等，E表示数学期望。

在图像生成领域，生成对抗网络已被广泛应用于多种任务\cite{chen2022generative,singh2021medical,zhou2023hybrid}，包括图像合成、图像修复、风格转换和图像生成等。由于GAN独特的训练方式，使其不仅仅在生成新图像方面表现出色，还可以用于图像特征提取\cite{xu2022te}。通过对原始图像进行特征提取，GAN的生成器可以生成与原始图像特征分布类似的新图片。这种方法已被成功应用于多个领域，例如医学成像和计算机视觉。因此，生成对抗网络在图像生成和特征提取方面的广泛应用将在未来的研究中得到进一步的探索和发展。

%%\section{模型窃取攻击}
%
%自DNN在各个领域取得巨大成功以来，针对DNN模型的攻击就层出不穷，按照攻击方式的不同，可以分为以下三类\cite{xue2021intellectual}：
%（1）模型修改攻击。指常见的模型修改，主要包括包括模型微调，模型剪枝，模型压缩，模型再训练等方式。
%（2）删除攻击。指攻击者试图逃避水印或指纹的检测，主要包括删除攻击，篡改攻击，逆向工程攻击等方式。
%（3）主动攻击。指攻击者主动攻击和强攻击，主要包括歧义攻击，水印和指纹覆盖攻击，查询修改攻击等方式。
%
%\subsection{模型修改攻击}
%
%模型盗窃者在盗窃DNN模型后，通常会对DNN模型进行修改或者压缩，然后部署模型作为MLaas来非法盈利。模型修改主要包括：
%
%1）模型微调。微调通常用于迁移学习中，包括在源模型的基础上，根据自己定制的任务，继续训练模型，使得DNN模型在保持性能的同时修改内部的参数。模型微调可以从源模型派生出非常多的模型。由于内部参数发生改变，水印等可能也会随之变化，因此这对水印的鲁棒性是一个考验。
%	
%2）模型剪枝。由于DNN模型通常内存占用多，计算开销大，因此模型剪枝是在小型设备上部署DNN模型的常用方法。但是模型盗窃者可能会利用剪枝来删除水印，因此有效的水印技术应该能够抵御由模型剪枝引起的参数变化。
%	
%3）模型压缩。模型压缩可以显著降低DNN模型的内存需求和计算开销，常用的方法是知识蒸馏，通过将大型模型包含的知识转移到小模型上来达到模型压缩的目的。
%	
%4）模型再训练\cite{namba2019robust}。模型再训练是一种很直接的方法，这样能尽可能的去除或者减少原有水印的影响，相应的，这种攻击方式成本也比较高。
%
%
%\subsection{删除攻击}
%
%目前大部分DNN模型的知识产权保护工作专注于水印对DNN模型被修改时的鲁棒性，而很少考虑水印或指纹本身受到的攻击。删除攻击主要包括：
%
%1）删除攻击\cite{shafieinejad2021robustness}。攻击者试图修改模型以删除原有的水印。
%
%2）篡改攻击。攻击者知道DNN模型中存在水印，试图窜改模型来删除原有的水印和指纹特征。
%
%3）逆向工程攻击\cite{fan2019rethinking}。如果攻击者知道并可以获得原始训练数据，可能会直接对内部参数进行逆向工程。
%
%
%Shafieinejad等人\cite{shafieinejad2021robustness}研究了DNN中基于后门的水印方法的移除攻击，表明攻击者可以仅依靠公共数据集删除水印，而不用访问训练集和模型参数。还提出了一种检验水印的方法，表明基于后门的水印不够安全，无法保持水印的隐藏。
%
%
%\subsection{主动攻击}
%
%除了被动的攻击方式，攻击者还可能对DNN模型发动更强的主动攻击。主动攻击主要包括：
%
%1）歧义攻击。歧义攻击指在DNN模型上伪造额外的水印来混淆所有权的验证。研究表明，除非采取不可逆的水印方案，否者即使是鲁棒性的水印，也不一定能验证模型的所有权\cite{fan2019rethinking}。
%
%2）水印覆盖攻击\cite{darvish2019deepsigns, chen2019blackmarks, chen2019deepmarks}。即使攻击者不知道具体的私有水印信息，但他知道模型水印嵌入的方法，就可能通过在DNN模型中嵌入新的水印来覆盖原有的水印，从未破坏原有的水印使其不可读。
%
%3）查询修改攻击。攻击者修改查询结果来使得水印验证过程无效。一个典型的方式是攻击者获得DNN模型并部署为MLaaS后，会主动检测一个查询是否为水印验证查询，从而修改或者屏蔽该查询，使水印验证无效。



\section{模型知识产权保护}

为了训练一个高性能的神经网络模型，需要该领域专家的先验知识来设计模型结构，大量的训练数据、昂贵的计算资源和漫长的训练时间。因此，训练后的DNN模型属于模型所有者的知识产权。由于DNN模型在各个领域都得到了高效的应用，许多不法分子开始盗窃、复制和修改这些模型以获利。为了保护神经网络模型的知识产权\cite{JFYZ202205002,WXAQ202202001}，许多学者受多媒体数字水印的启发，使用模型水印和模型指纹来验证DNN模型的所有权。

\subsection{模型水印}

模型水印是第一种被提出的保护DNN模型知识产权的方法，根据水印嵌入方式和提取方式的不同，主要分为白盒水印和黑盒水印。

\noindent\textbf{（1） \  白盒水印   }

在白盒场景下，模型所有者可以利用模型的全部知识构造水印，这些知识包括训练数据集，训练方法，模型内部权重参数和结构。Kuribayashi等人\cite{kuribayashi2020deepwatermark}提出一种可量化的水印嵌入方法，该方法基于全连接层的权重，为了使嵌入水印不对模型造成太大影响，通过改变训练中的参数来量化水印的影响。不同于基于权重的方法，基于内部结构的水印方法抵抗模型修改的鲁棒性更强。在DNN模型中，可以改变模型的结构，添加额外的一层作为护照\cite{fan2019rethinking}，以此来作为数字签名，增加模型的安全性。当不法分子发起歧义攻击，尝试嵌入额外水印时，模型性能会急剧下降。然而，这种方式的部署代价非常高。

\noindent\textbf{（2） \ 黑盒水印   }

在黑盒场景下，盗窃模型的内部结构和权重参数等是未知的，模型所有者只能通过其提供的API服务进行水印验证。一般而言，黑盒水印通过构造特殊的触发集实现，主要有以下几种方式：

1）通过更改样本标签构造触发集，将原始样本标签更改为模型所有者指定的与原始内容不符合的标签，这样仅修改标签不做任何其他修改的水印方法称为零位水印。

2）通过在原始样本中嵌入额外水印信息和更改标签构造触发集，这样可以在模型输出中嵌入模型所有者的版权信息。

3）通过添加新的样本构造触发集，这样的方式对模型的精度影响较大，一般通过模型微调最大限度的减少新样本对模型决策的影响。



\subsection{模型指纹}

模型指纹一般是利用模型本身来寻找和提取一些固有的特征来作为指纹。相较于模型水印的方法，模型指纹一般不对模型进行修改，因此不会影响模型的精度。一般来说，可以选择靠近决策边界的对抗性样本作为模型的指纹特征，来验证模型所有权。模型指纹分为指纹生成和指纹验证两个阶段。

\noindent\textbf{（1） \  模型指纹生成   }

指纹生成是模型指纹技术中的第一阶段，它需要选择一组样本来生成指纹。模型指纹与生物学上的指纹类似，具有唯一标识性，用以标记DNN模型的所有权。根据之前对指纹的相关研究\cite{zhao2020afa,lukas2019deep,cao2021ipguard}，可以把靠近决策边界的对抗性样本作为模型的指纹。这些样本可以通过将一个已知标签的样本加上一些扰动来生成，从而欺骗模型产生错误的分类结果。具体来说，通过将此类样本输入DNN模型，将其对应的输出标签作为模型的指纹标签。

对于一个正常样本$x$，添加一个微小的扰动$\delta$，使得第$i$个输出满足：
\begin{equation}
	\label{eq:4}
	arg \mathop{max} \limits_i g_i(x) = y \wedge arg \mathop{max} \limits_i g_i(x + \delta) = y'
\end{equation}

其中$y$表示输入样本$x$对真实标签，叠加扰动后输出$y' \neq y$，$\delta$是对抗扰动，$x + \delta$即为对抗样本。

所以一组对抗性样本作为指纹样本，对应的输出作为指纹标签，共同构成模型指纹。即$X' = \{x_1',x_2',...,x_n'\}$作为模型指纹样本，对应的预测结果$Y' = \{y_1', y_2',...,y_n'\}$为指纹标签。\\

\noindent\textbf{（2） \ 模型指纹验证   }

指纹验证是模型指纹技术中的第二个阶段。在这个阶段中，生成的指纹将被用于验证模型的所有权。验证过程基于相同的原理，即使用对抗性样本来检查模型的响应。具体来说，在模型指纹验证阶段，通常的做法是使用指纹样本查询可疑模型API，比较API返回的预测标签和指纹标签的匹配程度。

设定一个阈值$\tau$，当阈值标签和指纹标签的匹配成功率超过阈值$\tau$时，则视为匹配成功，判定提供指纹样本的人是模型的合法拥有者。

设$X' = \{x_1',x_2',...,x_n'\}$为$n$个指纹样本，$Y' = \{y_1', y_2',...,y_n'\}$为对应的指纹标签，将$n$个指纹样本输入可疑模型API后，预测标签为$\widetilde{Y}' = \{\widetilde{y_1}', \widetilde{y_2}',...,\widetilde{y_n}'\}$。其中预测标签与指纹标签相同的数量为$q$，即$y_i' = \widetilde{y_i}'(i = 1, 2,...,q)$。

定义验证函数如下：
\begin{equation}
	\label{eq:5}
	Verify(Y', \widetilde{Y}')= \left
	\{ 
	\begin{array}{ll} 
		1, &\quad \dfrac{q}{n} \geq \tau \\ 
		\\
		0, &\quad \mbox{其他}\\ 
	\end{array} 
	\right.
\end{equation}

式\ref{eq:5}中，$q/n$表示匹配成功率，$\tau$是判定阈值。当匹配成功率大于等于阈值$\tau$时，结果为$1$，表示指纹成功匹配，判定可疑模型为盗版模型。


\section{本章小结}

本章主要介绍了深度神经网络，对抗性攻击，生成对抗网络和知识产权保护这四个方面的相关概念和理论基础。深度神经网络复杂的结构使其训练昂贵耗时，但是一旦训练完成就可以快速处理新数据。对抗性样本是一种人类肉眼无法察觉到变化而会导致模型输出错误的特殊样本，根据方式的不同，有多种生成的方法。本文利用对抗性样本靠近模型分类边界的特点构造初始近边界数据。生成对抗网络具有强大的特征提取能力，可以利用其生成与训练样本特征分布类似的新样本。本文生成对抗网络提取初始近边界数据的特征，然后使用生成器生成新的、私有的近边界数据。在神经网络模型知识产权保护领域，目前使用最广泛的是模型水印和指纹这两种方法，它们大多基于验证模型所有权的思路进行设计与实现。
